{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jweO8gnJzXTF"
      },
      "outputs": [],
      "source": [
        "\n",
        "!wget https://dlcdn.apache.org/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!tar -xvzf spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!ls /content/spark-3.2.1-bin-hadoop3.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "O6DceTwv1edX",
        "outputId": "386873bd-14db-4b6d-8ea4-eddc4de49c7d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found existing installation: pyspark 3.2.0\n",
            "Uninstalling pyspark-3.2.0:\n",
            "  Would remove:\n",
            "    /usr/local/bin/beeline\n",
            "    /usr/local/bin/beeline.cmd\n",
            "    /usr/local/bin/docker-image-tool.sh\n",
            "    /usr/local/bin/find-spark-home\n",
            "    /usr/local/bin/find-spark-home.cmd\n",
            "    /usr/local/bin/find_spark_home.py\n",
            "    /usr/local/bin/load-spark-env.cmd\n",
            "    /usr/local/bin/load-spark-env.sh\n",
            "    /usr/local/bin/pyspark\n",
            "    /usr/local/bin/pyspark.cmd\n",
            "    /usr/local/bin/pyspark2.cmd\n",
            "    /usr/local/bin/run-example\n",
            "    /usr/local/bin/run-example.cmd\n",
            "    /usr/local/bin/spark-class\n",
            "    /usr/local/bin/spark-class.cmd\n",
            "    /usr/local/bin/spark-class2.cmd\n",
            "    /usr/local/bin/spark-shell\n",
            "    /usr/local/bin/spark-shell.cmd\n",
            "    /usr/local/bin/spark-shell2.cmd\n",
            "    /usr/local/bin/spark-sql\n",
            "    /usr/local/bin/spark-sql.cmd\n",
            "    /usr/local/bin/spark-sql2.cmd\n",
            "    /usr/local/bin/spark-submit\n",
            "    /usr/local/bin/spark-submit.cmd\n",
            "    /usr/local/bin/spark-submit2.cmd\n",
            "    /usr/local/bin/sparkR\n",
            "    /usr/local/bin/sparkR.cmd\n",
            "    /usr/local/bin/sparkR2.cmd\n",
            "    /usr/local/lib/python3.7/dist-packages/pyspark-3.2.0.dist-info/*\n",
            "    /usr/local/lib/python3.7/dist-packages/pyspark/*\n",
            "Proceed (y/n)? "
          ]
        }
      ],
      "source": [
        "!pip uninstall pyspark\n",
        "!pip install pyspark==3.0.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pubs9zux-7lz"
      },
      "outputs": [],
      "source": [
        "!pip install findspark\n",
        "!pip install elephas\n",
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UZMVyiD3xIFh"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.1-bin-hadoop3.2\"\n",
        "import findspark\n",
        "findspark.init()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pv-j66tmi8e1"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ll1svhL8xKm8"
      },
      "outputs": [],
      "source": [
        "# Spark Session, Pipeline, Functions, and Metrics\n",
        "from pyspark import SparkContext, SparkConf\n",
        "from pyspark.sql import SQLContext\n",
        "# from pyspark.ml.feature import OneHotEncoderEstimator, StringIndexer, StandardScaler, VectorAssembler\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.sql.functions import rand\n",
        "from pyspark.mllib.evaluation import MulticlassMetrics\n",
        "\n",
        "# Keras / Deep Learning\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Dense, Dropout, Activation\n",
        "from keras import optimizers, regularizers\n",
        "from tensorflow.keras.optimizers import Adam \n",
        "\n",
        "import tensorflow.keras.backend as K\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense,Embedding,Conv1D,MaxPooling1D,LSTM\n",
        "from sklearn.metrics import accuracy_score,confusion_matrix,classification_report\n",
        "# Elephas for Deep Learning on Spark\n",
        "from elephas.ml_model import ElephasEstimator\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import numpy as np\n",
        " \n",
        "# for reproducibility\n",
        "\n",
        "\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Embedding, Dropout, SpatialDropout1D\n",
        "from keras.layers import LSTM, Conv1D, MaxPooling1D\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PZOJT3T0xUuj"
      },
      "outputs": [],
      "source": [
        "# Spark Session\n",
        "conf = SparkConf().setAppName('Spark DL Tabular Pipeline').setMaster('local[6]')\n",
        "sc = SparkContext(conf=conf)\n",
        "sql_context = SQLContext(sc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9HlDtOF9xajI"
      },
      "outputs": [],
      "source": [
        "# # Load Data to Spark Dataframe\n",
        "# df = sql_context.read.csv('/bank.csv',\n",
        "#                     header=True,\n",
        "#                     inferSchema=True)\n",
        "expr_name = 'expr_13'\n",
        "look_back = 24*120 # 60 days, as each entry is for 1 hour\n",
        "lstm_layers = 64\n",
        "epochs=5\n",
        "batch_size=64\n",
        "num_of_features = 5\n",
        "num_classes = 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HReG5eWxbjfn",
        "outputId": "814adde0-cca9-40ed-e60f-d113eaeabf1c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(22420, 9)"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "csv = pd.read_csv(\"/content/df_data.csv\")\n",
        "csv.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319
        },
        "id": "ngi1HlXXe-Lt",
        "outputId": "b1929947-e2f1-4e5a-ac24-71433f288058"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-a63bdb06-6643-43a9-89cd-3c353806778c\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>tweets</th>\n",
              "      <th>cleaned_tweets</th>\n",
              "      <th>date_clean</th>\n",
              "      <th>crypto_sentiment</th>\n",
              "      <th>subjectivity</th>\n",
              "      <th>polarity</th>\n",
              "      <th>sentiment</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>nice project \\n\\n\\n\\n\\n\\nortcoin ort okratech ...</td>\n",
              "      <td>nice project ortcoin ort okratech bitcoin aird...</td>\n",
              "      <td>2021-06-22</td>\n",
              "      <td>positive</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.600</td>\n",
              "      <td>positive</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>Long Bitcoin short the banks üßê</td>\n",
              "      <td>Long Bitcoin short bank</td>\n",
              "      <td>2021-08-25</td>\n",
              "      <td>negative</td>\n",
              "      <td>0.35</td>\n",
              "      <td>-0.025</td>\n",
              "      <td>negative</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>Top Trending Cryptocurrency Post - DOGECOIN Se...</td>\n",
              "      <td>Top Trending Cryptocurrency Post DOGECOIN Sell...</td>\n",
              "      <td>2021-07-02</td>\n",
              "      <td>negative</td>\n",
              "      <td>0.40</td>\n",
              "      <td>0.250</td>\n",
              "      <td>positive</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>Can one expect another wave of BTC's decline s...</td>\n",
              "      <td>Can one expect another wave BTC decline soon v...</td>\n",
              "      <td>2021-07-24</td>\n",
              "      <td>negative</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.000</td>\n",
              "      <td>neutral</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>We will see‚Ä¶\\n\\nbitcoin btc bnb band bake $btc...</td>\n",
              "      <td>We see bitcoin btc bnb band bake btc dCc xZ dP</td>\n",
              "      <td>2021-05-29</td>\n",
              "      <td>positive</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.000</td>\n",
              "      <td>neutral</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a63bdb06-6643-43a9-89cd-3c353806778c')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-a63bdb06-6643-43a9-89cd-3c353806778c button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-a63bdb06-6643-43a9-89cd-3c353806778c');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "   Unnamed: 0                                             tweets  \\\n",
              "0           0  nice project \\n\\n\\n\\n\\n\\nortcoin ort okratech ...   \n",
              "1           1                     Long Bitcoin short the banks üßê   \n",
              "2           2  Top Trending Cryptocurrency Post - DOGECOIN Se...   \n",
              "3           3  Can one expect another wave of BTC's decline s...   \n",
              "4           4  We will see‚Ä¶\\n\\nbitcoin btc bnb band bake $btc...   \n",
              "\n",
              "                                      cleaned_tweets  date_clean  \\\n",
              "0  nice project ortcoin ort okratech bitcoin aird...  2021-06-22   \n",
              "1                            Long Bitcoin short bank  2021-08-25   \n",
              "2  Top Trending Cryptocurrency Post DOGECOIN Sell...  2021-07-02   \n",
              "3  Can one expect another wave BTC decline soon v...  2021-07-24   \n",
              "4     We see bitcoin btc bnb band bake btc dCc xZ dP  2021-05-29   \n",
              "\n",
              "  crypto_sentiment  subjectivity  polarity sentiment  target  \n",
              "0         positive          1.00     0.600  positive    True  \n",
              "1         negative          0.35    -0.025  negative    True  \n",
              "2         negative          0.40     0.250  positive   False  \n",
              "3         negative          0.00     0.000   neutral   False  \n",
              "4         positive          0.00     0.000   neutral   False  "
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "csv.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ojdMwUdO6vsN"
      },
      "outputs": [],
      "source": [
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM, GRU\n",
        "from elephas.spark_model import SparkModel\n",
        "from elephas.utils.rdd_utils import to_simple_rdd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UuF6ocoMiiWR",
        "outputId": "4070f2e4-739f-40a6-c1d8-4abe9e70131a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "parsing CSV\n"
          ]
        }
      ],
      "source": [
        "print('parsing CSV')\n",
        "\n",
        "X, Y = [], []\n",
        "\n",
        "for index, row in csv.iterrows():\n",
        "    X.append(row[2])\n",
        "    y_part = row[4]\n",
        "    if y_part == \"negative\":\n",
        "        yy = np.array([0])\n",
        "    elif y_part == \"positive\":\n",
        "        yy = np.array([1])\n",
        "    # else:\n",
        "    #     raise Exception('Invalid y_part value=' + y_part)\n",
        "    Y.append(yy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RbKqDbM7lSgT",
        "outputId": "77f076b1-9500-40d5-dca7-e43283487a9e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[    0,     0,     0, ...,  1735,     3, 13342],\n",
              "       [    0,     0,     0, ...,     1,   116,   327],\n",
              "       [    0,     0,     0, ..., 13343,     7,     1],\n",
              "       ...,\n",
              "       [    0,     0,     0, ...,   130,     2,     1],\n",
              "       [    0,     0,     0, ...,  8883,   172, 11512],\n",
              "       [    0,     0,     0, ...,    78, 41121,   119]], dtype=int32)"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KUNKw11QjF1X"
      },
      "outputs": [],
      "source": [
        "print('build words map')\n",
        "\n",
        "max_features = 50000\n",
        "tokenizer = Tokenizer(num_words=max_features)\n",
        "tokenizer.fit_on_texts(X)\n",
        "X = tokenizer.texts_to_sequences(X)\n",
        "X = pad_sequences(X)\n",
        "X, Xt, Y, Yt = train_test_split(X, Y, test_size = 0.3, random_state = random.set_seed(1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KPMPMQxOs8mc"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "from tensorflow.random import set_seed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GIwUi3Twlgv5"
      },
      "outputs": [],
      "source": [
        "X, Xt, Y, Yt = train_test_split(X, Y, test_size = 0.3, random_state = random.seed(1))\n",
        "validation_size = 1500\n",
        "X_validate = Xt[-validation_size:]\n",
        "Y_validate = Yt[-validation_size:]\n",
        "Xt = Xt[:-validation_size]\n",
        "Yt = Yt[:-validation_size]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ToTJdiOAtbOM"
      },
      "outputs": [],
      "source": [
        "maxlen = 0\n",
        "def wrap_array(x, maxlen):\n",
        "    for index in range(len(x)):\n",
        "        xx = x[index]\n",
        "        if len(xx) > maxlen:\n",
        "            maxlen = len(xx)\n",
        "        x[index] = np.array(xx)\n",
        "    return np.array(x), maxlen\n",
        "\n",
        "X, maxlen = wrap_array(X, maxlen)\n",
        "Xt, maxlen = wrap_array(Xt, maxlen)\n",
        "X_validate, maxlen = wrap_array(X_validate, maxlen)\n",
        "Y, maxlen = wrap_array(Y, maxlen)\n",
        "Yt, maxlen = wrap_array(Yt, maxlen)\n",
        "Y_validate, maxlen = wrap_array(Y_validate, maxlen)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TW-QEM7omews"
      },
      "source": [
        "**Insert Model Here**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7RQqgXTLx-YT",
        "outputId": "5ca28db3-fd04-4819-f56d-f35b2289f2a3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "build model\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 55, 128)           6400000   \n",
            "                                                                 \n",
            " spatial_dropout1d (SpatialD  (None, 55, 128)          0         \n",
            " ropout1D)                                                       \n",
            "                                                                 \n",
            " conv1d (Conv1D)             (None, 55, 32)            12320     \n",
            "                                                                 \n",
            " max_pooling1d (MaxPooling1D  (None, 27, 32)           0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 124)               77872     \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 125       \n",
            "                                                                 \n",
            " activation (Activation)     (None, 1)                 0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 6,490,317\n",
            "Trainable params: 6,490,317\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "print('build model')\n",
        "\n",
        "batch_size = 256\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_features, 128, input_length=maxlen))\n",
        "model.add(SpatialDropout1D(0.2))\n",
        "model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
        "model.add(MaxPooling1D(pool_size=2))\n",
        "model.add(LSTM(124, dropout=0.2, recurrent_dropout=0.2))\n",
        "model.add(Dense(1))\n",
        "model.add(Activation('sigmoid'))\n",
        "\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='nadam',\n",
        "              metrics=['accuracy'])\n",
        "print(model.summary())\n",
        "\n",
        "# print('Train...')\n",
        "# model.fit(X, Y, batch_size=batch_size, epochs=2, validation_data=(Xt, Yt), verbose=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1rB5XZ0-tvd9"
      },
      "outputs": [],
      "source": [
        "rdd = to_simple_rdd(sc, Xt, Yt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A3TxhppWuDWe"
      },
      "outputs": [],
      "source": [
        "spark_model = SparkModel(model, frequency='epoch', num_workers=5, mode='synchronous')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d3IkAsruuWJq"
      },
      "outputs": [],
      "source": [
        "from pyspark import SparkContext, SparkConf\n",
        "from keras.utils import np_utils\n",
        "import time\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "winQPxuG1ChF",
        "outputId": "f77070a7-b9ff-4278-8361-6279ee08196c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: py4j in /usr/local/lib/python3.7/dist-packages (0.10.9.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install py4j"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 414
        },
        "id": "cccln03_uL-2",
        "outputId": "dd53693b-82ea-4c30-c892-50fc519fbdb3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ">>> Fit model\n"
          ]
        },
        {
          "ename": "Py4JError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-8cfac533aa88>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Train Spark model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mspark_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mend_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Evaluate Spark model by evaluating the underlying model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/elephas/spark_model.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, rdd, **kwargs)\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'>>> Fit model'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m             \u001b[0mrdd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepartition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'asynchronous'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'synchronous'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'hogwild'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36mrepartition\u001b[0;34m(self, numPartitions)\u001b[0m\n\u001b[1;32m   2398\u001b[0m          \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2399\u001b[0m         \"\"\"\n\u001b[0;32m-> 2400\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoalesce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumPartitions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2401\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2402\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcoalesce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumPartitions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36mcoalesce\u001b[0;34m(self, numPartitions, shuffle)\u001b[0m\n\u001b[1;32m   2418\u001b[0m             \u001b[0mselfCopy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reserialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mser\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2419\u001b[0m             \u001b[0mjrdd_deserializer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselfCopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2420\u001b[0;31m             \u001b[0mjrdd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselfCopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoalesce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumPartitions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2421\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2422\u001b[0m             \u001b[0mjrdd_deserializer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36m_jrdd\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2950\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2951\u001b[0m         wrapped_func = _wrap_function(self.ctx, self.func, self._prev_jrdd_deserializer,\n\u001b[0;32m-> 2952\u001b[0;31m                                       self._jrdd_deserializer, profiler)\n\u001b[0m\u001b[1;32m   2953\u001b[0m         python_rdd = self.ctx._jvm.PythonRDD(self._prev_jrdd.rdd(), wrapped_func,\n\u001b[1;32m   2954\u001b[0m                                              self.preservesPartitioning, self.is_barrier)\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36m_wrap_function\u001b[0;34m(sc, func, deserializer, serializer, profiler)\u001b[0m\n\u001b[1;32m   2828\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mserializer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"serializer should not be empty\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2829\u001b[0m     \u001b[0mcommand\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprofiler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeserializer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2830\u001b[0;31m     \u001b[0mpickled_command\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbroadcast_vars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mincludes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_prepare_for_python_RDD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2831\u001b[0m     return sc._jvm.PythonFunction(bytearray(pickled_command), env, includes, sc.pythonExec,\n\u001b[1;32m   2832\u001b[0m                                   sc.pythonVer, broadcast_vars, sc._javaAccumulator)\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36m_prepare_for_python_RDD\u001b[0;34m(sc, command)\u001b[0m\n\u001b[1;32m   2815\u001b[0m     \u001b[0mser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCloudPickleSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2816\u001b[0m     \u001b[0mpickled_command\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2817\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpickled_command\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetBroadcastThreshold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Default 1M\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2818\u001b[0m         \u001b[0;31m# The broadcast will have same life cycle as created PythonRDD\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2819\u001b[0m         \u001b[0mbroadcast\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpickled_command\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1708\u001b[0m             message = compute_exception_message(\n\u001b[1;32m   1709\u001b[0m                 \"{0} does not exist in the JVM\".format(name), error_message)\n\u001b[0;32m-> 1710\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mPy4JError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1711\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1712\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mPy4JError\u001b[0m: PythonUtils does not exist in the JVM"
          ]
        }
      ],
      "source": [
        "# Train Spark model\n",
        "start_time = time.time()\n",
        "spark_model.fit(rdd, epochs=2, batch_size=batch_size, verbose=10, validation_split=0.1)\n",
        "end_time = time.time()\n",
        "# Evaluate Spark model by evaluating the underlying model\n",
        "score = spark_model.evaluate(Xt, Yt, verbose=2)\n",
        "# print('Test accuracy:', score[1])\n",
        "\n",
        "# np.savetxt('results.txt', [end_time-start_time, score[1]], fmt=\"%f\")\n",
        "# spark_model.save('mnist_model.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bNqk2NLYx9Vx"
      },
      "source": [
        "**Data Preprocessing and Deep Learning Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KhR0gQQsx8l2"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Build RDD from numpy features and labels\n",
        "rdd = to_simple_rdd(sc, x_train, y_train)\n",
        "\n",
        "# Initialize SparkModel from tensorflow.keras model and Spark context\n",
        "spark_model = SparkModel(model, frequency='epoch', num_workers=5, mode='synchronous')\n",
        "\n",
        "# Train Spark model\n",
        "start_time = time.time()\n",
        "spark_model.fit(rdd, epochs=epochs, batch_size=batch_size, verbose=10, validation_split=0.1)\n",
        "end_time = time.time()\n",
        "# Evaluate Spark model by evaluating the underlying model\n",
        "score = spark_model.evaluate(x_test, y_test, verbose=2)\n",
        "print('Test accuracy:', score[1])\n",
        "\n",
        "np.savetxt('results.txt', [end_time-start_time, score[1]], fmt=\"%f\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKEXf5nmyLOQ"
      },
      "source": [
        "**DL Model ends, Add Elephas**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dZoXaUVhx7-C"
      },
      "outputs": [],
      "source": [
        "# Set and Serialize Optimizer\n",
        "optimizer_conf = optimizers.Adam(lr=0.01)\n",
        "opt_conf = optimizers.serialize(optimizer_conf)\n",
        "\n",
        "# Initialize SparkML Estimator and Get Settings\n",
        "estimator = ElephasEstimator()\n",
        "estimator.setFeaturesCol(\"features\")\n",
        "estimator.setLabelCol(\"label_index\")\n",
        "estimator.set_keras_model_config(model.to_yaml())\n",
        "estimator.set_categorical_labels(True)\n",
        "estimator.set_nb_classes(nb_classes)\n",
        "estimator.set_num_workers(1)\n",
        "estimator.set_epochs(25) \n",
        "estimator.set_batch_size(64)\n",
        "estimator.set_verbosity(1)\n",
        "estimator.set_validation_split(0.10)\n",
        "estimator.set_optimizer_config(opt_conf)\n",
        "estimator.set_mode(\"synchronous\")\n",
        "estimator.set_loss(\"binary_crossentropy\")\n",
        "estimator.set_metrics(['acc'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "66oPOyHjy5kB"
      },
      "outputs": [],
      "source": [
        "# Create Deep Learning Pipeline\n",
        "dl_pipeline = Pipeline(stages=[estimator])\n",
        "def dl_pipeline_fit_score_results(dl_pipeline=dl_pipeline,\n",
        "                                  train_data=train_data,\n",
        "                                  test_data=test_data,\n",
        "                                  label='label_index'):\n",
        "    \n",
        "    fit_dl_pipeline = dl_pipeline.fit(train_data)\n",
        "    pred_train = fit_dl_pipeline.transform(train_data)\n",
        "    pred_test = fit_dl_pipeline.transform(test_data)\n",
        "    \n",
        "    pnl_train = pred_train.select(label, \"prediction\")\n",
        "    pnl_test = pred_test.select(label, \"prediction\")\n",
        "    \n",
        "    pred_and_label_train = pnl_train.rdd.map(lambda row: (row[label], row['prediction']))\n",
        "    pred_and_label_test = pnl_test.rdd.map(lambda row: (row[label], row['prediction']))\n",
        "    \n",
        "    metrics_train = MulticlassMetrics(pred_and_label_train)\n",
        "    metrics_test = MulticlassMetrics(pred_and_label_test)\n",
        "    \n",
        "    print(\"Training Data Accuracy: {}\".format(round(metrics_train.precision(),4)))\n",
        "    print(\"Training Data Confusion Matrix\")\n",
        "    display(pnl_train.crosstab('label_index', 'prediction').toPandas())\n",
        "    \n",
        "    print(\"\\nTest Data Accuracy: {}\".format(round(metrics_test.precision(),4)))\n",
        "    print(\"Test Data Confusion Matrix\")\n",
        "    display(pnl_test.crosstab('label_index', 'prediction').toPandas())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b9o4b6Wiy_TM"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "TensorflowSpark.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}